<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>¬°ARREGLADO CON CAPAS (Z-INDEX)!</title>
  
  <style>
    body { 
      margin: 0; 
      overflow: hidden; 
      background: #000; 
    }
    
    /* ----- ¬°AQU√ç EST√Å LA NUEVA CORRECCI√ìN! ----- */
    /* 1. Oculta el <video> original a la fuerza */
    video {
      display: none !important;
      z-index: -1; /* M√°ndalo al fondo */
    }
    
    /* 2. Trae el <canvas> de p5.js al frente */
    canvas {
      z-index: 100; /* Ponlo encima de todo */
    }
    /* ----------------------------------------- */

  </style>
</head>

<body>
  <script src="https://unpkg.com/ml5@0.12.2/dist/ml5.min.js"></script>
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"></script>

  <script>
    let videoEl;
    let faceapi;
    let detections = [];
    let modeloListo = false;

    function setup() {
      const w = windowWidth < 700 ? windowWidth : 640;
      const h = windowHeight < 500 ? windowHeight : 480;
      createCanvas(w, h); // p5.js crea un <canvas>

      videoEl = createCapture({
        video: { facingMode: "user", width: w, height: h },
        audio: false
      }); // p5.js crea un <video>

      videoEl.elt.onloadedmetadata = () => {
        console.log("üé• C√°mara lista (metadata cargada)");
      }

      videoEl.size(w, h);
      // No necesitamos .hide() ni .style, el CSS se encarga.

      const options = {
        withLandmarks: true,
        withExpressions: false,
        withDescriptors: false
      };

      faceapi = ml5.faceApi(videoEl.elt, options, modelReady);

      textAlign(CENTER);
      textSize(20);
      fill(255);
    }

    function modelReady() {
      console.log("‚úÖ Modelo cargado correctamente");
      modeloListo = true;
      faceapi.detect(videoEl.elt, gotResults);
    }

    function gotResults(err, result) {
      if (err) {
        console.error(err);
        return;
      }
      detections = result;
      faceapi.detect(videoEl.elt, gotResults);
    }

    function draw() {
      background(0); // <-- Pinta el canvas de negro

      if (videoEl.width === 0) {
        fill(255);
        text("üé• Iniciando c√°mara...", width / 2, height / 2);
        return;
      }

      if (!modeloListo) {
        fill(255);
        text("üß† Cargando modelo de rostro...", width / 2, height / 2);
        return;
      }

      // Dibuja el video EN el canvas
      push();
      translate(width, 0);
      scale(-1, 1);
      image(videoEl, 0, 0, width, height);
      pop();

      // Dibuja los puntos o el texto
      if (detections.length > 0) {
        const puntos = detections[0].landmarks.positions;
        noStroke();
        fill(255, 100, 150, 160);
        for (let p of puntos) {
          ellipse(width - p.x, p.y, 8, 8);
        }
      } else {
        fill(255);
        text("Mira hacia la c√°mara üëÄ", width / 2, height / 2);
      }
    }

    function windowResized() {
      resizeCanvas(windowWidth < 700 ? windowWidth : 640, windowHeight < 500 ? windowHeight : 480);
    }
  </script>
</body>
</html>
