<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>¬°DESPIERTA C√ÅMARA!</title>
  
  <style>
    body { 
      margin: 0; 
      overflow: hidden; 
      background: #000; 
    }
    
    /* Manda el <video> original al fondo */
    video {
      z-index: -1;
      display: none !important;
    }
    
    /* Trae el <canvas> de p5.js al frente */
    canvas {
      z-index: 100;
    }
  </style>
</head>

<body>
  <script src="https://unpkg.com/ml5@0.12.2/dist/ml5.min.js"></script>
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"></script>

  <script>
    let videoEl;
    let faceapi;
    let detections = [];
    let modeloListo = false;

    function setup() {
      const w = windowWidth < 700 ? windowWidth : 640;
      const h = windowHeight < 500 ? windowHeight : 480;
      createCanvas(w, h); // p5.js crea un <canvas>

      videoEl = createCapture({
        video: { facingMode: "user", width: w, height: h },
        audio: false
      }); // p5.js crea un <video>

      videoEl.elt.onloadedmetadata = () => {
        console.log("üé• C√°mara lista (metadata cargada)");
        // ----- ¬°AQU√ç EST√Å LA NUEVA CORRECCI√ìN! -----
        // "Despertamos" la c√°mara para que env√≠e video
        videoEl.play(); 
        // ------------------------------------------
      }

      videoEl.size(w, h);
      // No necesitamos .hide(), el CSS se encarga.

      const options = {
        withLandmarks: true,
        withExpressions: false,
        withDescriptors: false
      };

      faceapi = ml5.faceApi(videoEl.elt, options, modelReady);

      textAlign(CENTER);
      textSize(20);
      fill(255);
    }

    function modelReady() {
      console.log("‚úÖ Modelo cargado correctamente");
      modeloListo = true;
      faceapi.detect(videoEl.elt, gotResults);
    }

    function gotResults(err, result) {
      if (err) {
        console.error(err);
        return;
      }
      detections = result;
      faceapi.detect(videoEl.elt, gotResults);
    }

    function draw() {
      // Esta comprobaci√≥n es m√°s robusta
      if (videoEl.elt.videoWidth === 0) {
        background(0); // Mantenemos el fondo negro
        fill(255);
        text("üé• Iniciando c√°mara...", width / 2, height / 2);
        return;
      }

      if (!modeloListo) {
        background(0); // Mantenemos el fondo negro
        fill(255);
        text("üß† Cargando modelo de rostro...", width / 2, height / 2);
        return;
      }
      
      // ¬°Aqu√≠ es donde la magia pasa!
      background(0); // 1. Fondo negro

      // 2. Dibuja el video (que ya no ser√° gris)
      push();
      translate(width, 0);
      scale(-1, 1);
      image(videoEl, 0, 0, width, height);
      pop();

      // 3. Dibuja los puntos o el texto
      if (detections.length > 0) {
        const puntos = detections[0].landmarks.positions;
        noStroke();
        fill(255, 100, 150, 160);
        for (let p of puntos) {
          ellipse(width - p.x, p.y, 8, 8);
        }
      } else {
        fill(255);
        text("Mira hacia la c√°mara üëÄ", width / 2, height / 2);
      }
    }

    function windowResized() {
      resizeCanvas(windowWidth < 700 ? windowWidth : 640, windowHeight < 500 ? windowHeight : 480);
    }
  </script>
</body>
</html>
